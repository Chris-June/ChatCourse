import React from 'react';
import { Lightbulb, AlertTriangle, BookOpen, ShieldCheck, Search, Link2, ListChecks } from 'lucide-react';
import LessonTemplate from '../../../../../components/layouts/LessonTemplate';
import KeyTakeaways from '../../../components/KeyTakeaways';
import HallucinationGame from './components/HallucinationGame';

// HallucinationGame moved to './components/HallucinationGame'

const Lesson1_3: React.FC = () => {
  const quizQuestions = [
    {
      questionText: 'What is an AI \'hallucination\'',
      options: [
        'A creative story generated by the AI.',
        'A factual error or nonsensical statement presented as fact.',
        'A feature that allows the AI to see images.',
        'A type of computer virus.'
      ],
      correctAnswer: 'A factual error or nonsensical statement presented as fact.',
      explanation: 'Hallucinations happen because the AI is a prediction engine. It strings together plausible-sounding tokens, but it doesn\'t have a true understanding or fact-checker, so it can sometimes invent facts, sources, or details that are incorrect.'
    },
    {
      questionText: 'Why do AI hallucinations occur?',
      options: [
        'Because the AI is trying to be creative.',
        'Because the AI has a bad sense of humor.',
        'Because the AI is predicting the next word based on patterns, not facts, and can sometimes generate plausible but untrue information.',
        'Because the user asked a trick question.'
      ],
      correctAnswer: 'Because the AI is predicting the next word based on patterns, not facts, and can sometimes generate plausible but untrue information.',
      explanation: 'The AI\'s goal is to create a statistically likely sequence of words. If the patterns in its training data lead to an incorrect statement that seems plausible, it will generate it. It lacks a ground truth or fact-checking mechanism.'
    },
    {
      questionText: 'How can you best mitigate or reduce AI hallucinations?',
      options: [
        'Asking the AI to be more creative.',
        'Providing specific context and grounding data in the prompt.',
        'Using shorter prompts.',
        'Typing in all caps.'
      ],
      correctAnswer: 'Providing specific context and grounding data in the prompt.',
      explanation: 'Grounding the AI with real data, like a document or transcript, forces it to base its answers on provided facts rather than making things up from patterns.'
    },
    {
      questionText: 'Which of the following is a common example of an AI hallucination?',
      options: [
        'The AI refusing to answer a question.',
        'The AI providing a correct summary of a long article.',
        'The AI inventing a fake academic paper or URL to support its claims.',
        'The AI asking for clarification.'
      ],
      correctAnswer: 'The AI inventing a fake academic paper or URL to support its claims.',
      explanation: 'This is a classic hallucination. The AI predicts what a citation *should* look like but invents the details because it does not have access to a real one.'
    },
    {
      questionText: 'True or False: If an AI sounds confident, its answer is more likely to be factually correct.',
      options: [
        'True',
        'False'
      ],
      correctAnswer: 'False',
      explanation: 'An AI\'s tone is just another part of its prediction. It can generate highly confident-sounding prose for completely fabricated information. Always verify critical information from a primary source.'
    }
    ,
    {
      questionText: 'Which is the best prompt to reduce hallucinations?',
      options: [
        '“Be creative and confident.”',
        '“Cite sources with URLs and mark any uncertainty explicitly.”',
        '“Write as fast as possible.”',
        '“Use emojis to show confidence.”'
      ],
      correctAnswer: '“Cite sources with URLs and mark any uncertainty explicitly.”',
      explanation: 'Requesting citations and explicit uncertainty nudges the model toward verifiable, cautious output.'
    },
    {
      questionText: 'Which is a common hallucination pattern?',
      options: [
        'Providing a valid source link',
        'Admitting it lacks context',
        'Inventing a plausible but fake citation or URL',
        'Returning an empty response'
      ],
      correctAnswer: 'Inventing a plausible but fake citation or URL',
      explanation: 'LLMs can fabricate sources that look real because they predict the “shape” of a citation.'
    },
    {
      questionText: 'What’s a reliable mitigation within this module’s scope?',
      options: [
        'Turning temperature to 2.0',
        'Grounding with provided context and using structured output requirements',
        'Asking for jokes to lighten the tone',
        'Using only very long prompts'
      ],
      correctAnswer: 'Grounding with provided context and using structured output requirements',
      explanation: 'Supplying relevant context and constraining format are practical, immediate levers.'
    }
  ];

  return (
    <LessonTemplate
      moduleNumber={1}
      lessonNumber={3}
      title="When AI Gets It Wrong (Hallucinations)"
      subtitle="Understanding and identifying AI-generated misinformation."
      quizQuestions={quizQuestions}
    >
      <section className="mb-6 bg-muted/30 border border-border rounded-xl p-4">
        <p className="text-xs text-muted-foreground mb-2">Estimated time: 10–15 minutes</p>
        <h4 className="text-sm font-semibold mb-2 text-foreground">What you'll learn</h4>
        <ul className="list-disc list-inside text-sm text-muted-foreground space-y-1">
          <li>What hallucinations are and why they happen</li>
          <li>How to spot signs of hallucination and overconfidence</li>
          <li>Ways to reduce hallucinations with grounding and structure</li>
          <li>Healthy verification habits for critical tasks</li>
        </ul>
      </section>
      <div className="bg-card p-6 rounded-xl border border-border">
        <h2 className="text-2xl font-semibold mb-4 text-card-foreground flex items-center">
          <Lightbulb className="w-6 h-6 mr-3 text-amber-400" />
          The Confident Robot Problem
        </h2>
        <p className="text-lg text-muted-foreground mb-4">
          You've seen that AI is a powerful prediction engine. But what happens when its predictions go wrong? This leads to one of the most important and sometimes frustrating concepts in AI: <strong>hallucinations</strong>.
        </p>
        <div className="bg-muted p-4 rounded-lg border border-border">
          <p className="text-lg text-center font-semibold text-foreground">
            "I am 99.9% confident that the capital of Australia is Sydney."
          </p>
        </div>
        <p className="text-muted-foreground mt-4">
          Think of it this way: the AI doesn't 'know' things. It only knows which words are likely to follow other words. Sometimes, the most 'likely' path leads to a dead end of made-up facts, fake statistics, or invented stories. Because it has no concept of 'truth,' it can't tell you it's making something up. It just keeps predicting.
        </p>

      <div className="p-4 bg-muted/30 rounded-md border mt-4">
        <h3 className="text-sm font-semibold mb-2 flex items-center"><AlertTriangle className="w-4 h-4 mr-2 text-rose-400"/> Myths vs. Reality</h3>
        <ul className="text-sm text-muted-foreground space-y-1">
          <li><strong>Myth:</strong> Confident tone = correctness. <strong>Reality:</strong> Tone is predicted like any other token.</li>
          <li><strong>Myth:</strong> Longer answer = more accurate. <strong>Reality:</strong> Length can inflate confidence without facts.</li>
          <li><strong>Myth:</strong> It “knows” sources. <strong>Reality:</strong> It predicts the <em>shape</em> of citations unless you provide them.</li>
        </ul>
      </div>

      <div className="p-4 bg-muted/20 rounded-md border mt-4">
        <h3 className="text-sm font-semibold mb-2 flex items-center"><Search className="w-4 h-4 mr-2 text-cyan-400"/> Red Flags to Spot Fast</h3>
        <ul className="list-disc list-inside text-sm text-muted-foreground space-y-1">
          <li>Very specific facts with no sources (dates, figures, legal details)</li>
          <li>Perfectly formatted but unverifiable citations/URLs</li>
          <li>Name or entity mashups (mixing people, places, or product lines)</li>
          <li>Inconsistent numbers across the same answer</li>
        </ul>
      </div>

      <div className="p-4 bg-card rounded-md border mt-4">
        <h3 className="text-sm font-semibold mb-2 flex items-center"><BookOpen className="w-4 h-4 mr-2 text-emerald-400"/> Classic Hallucination Patterns</h3>
        <ul className="list-disc list-inside text-sm text-muted-foreground space-y-1">
          <li><strong>Fabricated citations:</strong> realistic‑looking papers/URLs that don’t exist.</li>
          <li><strong>Wrong timelines:</strong> mixing years, event order, or release dates.</li>
          <li><strong>Entity conflation:</strong> merging details of similarly named people/companies.</li>
          <li><strong>Speculative specifics:</strong> confident numbers without a source.</li>
        </ul>
      </div>

      <div className="p-4 bg-card rounded-md border mt-4">
        <h3 className="text-sm font-semibold mb-2">Quick Check</h3>
        <ol className="list-decimal list-inside text-sm text-muted-foreground space-y-1">
          <li>Which red flag have you seen most often?</li>
          <li>How would you rewrite a vague question to make hallucination less likely?</li>
        </ol>
        <p className="text-xs text-muted-foreground mt-2">Tip: Ask the inline chat to grade your answers “pass/fail + one improvement.”</p>
      </div>
      </div>

      <HallucinationGame />

      <div className="bg-card p-6 rounded-xl border border-border">
        <h2 className="text-2xl font-semibold mb-4 text-card-foreground flex items-center">
          Why Do They Really Happen?
        </h2>
        <p className="text-muted-foreground mb-4">
          Hallucinations are a direct side effect of the AI's core function: predicting the next word. The AI isn't consulting a knowledge base; it's weaving together words based on statistical patterns from its training data.
        </p>
        <p className="text-muted-foreground">
          Imagine an actor who forgets their lines but is determined to keep the play going. Instead of stopping, they improvise something that sounds plausible in the context of the scene. The AI does the same—it fills in gaps with what's statistically likely, which isn't always what's factually true.
        </p>

      <div className="p-4 bg-muted/20 rounded-md border mt-4">
        <h3 className="text-sm font-semibold mb-2">Probability in Action</h3>
        <p className="text-sm text-muted-foreground mb-2">When facts are missing, the model still has to pick a next token. It chooses what seems most <em>probable</em>—not what is most <em>true</em>.</p>
        <p className="text-xs text-muted-foreground">Takeaway: if you don’t provide context, you’re inviting plausible guesses.</p>
      </div>
      </div>

      <div className="bg-card p-6 rounded-xl border border-border">
        <h2 className="text-2xl font-semibold mb-4 text-card-foreground flex items-center">
          How to Mitigate Hallucinations
        </h2>
        <p className="text-muted-foreground mb-4">
          You can't eliminate hallucinations entirely, but you can significantly reduce their likelihood with good prompting habits:
        </p>
        <ul className="space-y-3 text-muted-foreground list-disc list-inside">
          <li><strong>Ask for Sources:</strong> Add phrases like "Cite your sources" or "Provide URLs for your claims." While the AI can hallucinate sources too, this often forces it to ground its response in more factual data.</li>
          <li><strong>Provide Grounding Context:</strong> Use the 'C' in the I.N.S.Y.N.C. framework. By giving the AI the specific text, data, or background information it needs, you anchor its predictions to your facts, not its own.</li>
          <li><strong>Request a Confidence Score:</strong> Ask the AI to "rate its confidence in this answer on a scale of 1 to 10." This can sometimes give you a signal about how speculative the response is.</li> 
          <li><strong>Use Structured Responses:</strong> While hallucinations still occur, there are improvements such as structured responses and other mechanisms we will discuss later in this course that help reduce errors and improve grounding.</li>
        </ul>

        <div className="rounded-md border p-3 bg-muted/40 mt-4">
          <p className="text-sm text-muted-foreground mb-2 font-medium flex items-center"><ShieldCheck className="w-4 h-4 mr-2"/> Return Uncertainty Explicitly</p>
          <pre className="bg-muted text-muted-foreground p-3 rounded-md overflow-x-auto text-xs">
{`If you are uncertain about any part, return:
{
  "certainty": "low|medium|high",
  "needs_review": true,
  "what_to_verify": ["...", "..."]
}`}
          </pre>
        </div>

        <div className="rounded-md border p-3 bg-muted/40 mt-4">
          <p className="text-sm text-muted-foreground mb-2 font-medium flex items-center"><Link2 className="w-4 h-4 mr-2"/> Ask for Verifiable Output</p>
          <pre className="bg-muted text-muted-foreground p-3 rounded-md overflow-x-auto text-xs">
{`Return sources as an array of objects with fields:
[{ "title": string, "url": string, "type": "primary|secondary" }]`}
          </pre>
        </div>

        <div className="rounded-md border p-3 bg-muted/30 mt-4">
          <p className="text-sm text-muted-foreground mb-2 font-medium flex items-center"><ListChecks className="w-4 h-4 mr-2"/> Verification Checklist</p>
          <ul className="list-disc list-inside text-sm text-muted-foreground space-y-1">
            <li>Scan numbers/dates; do they match elsewhere?</li>
            <li>Open 2–3 sources; do links resolve and corroborate?</li>
            <li>Flag any low-certainty fields for manual review.</li>
          </ul>
        </div>
      </div>

      <div className="p-4 bg-muted/20 rounded-md border mt-6">
        <h3 className="text-sm font-semibold mb-2">Mini‑Glossary</h3>
        <dl className="grid grid-cols-1 md:grid-cols-2 gap-3 text-sm text-muted-foreground">
          <div><dt className="font-medium">Hallucination</dt><dd>Fluent but false or unfounded output.</dd></div>
          <div><dt className="font-medium">Grounding</dt><dd>Anchoring answers to provided context or sources.</dd></div>
          <div><dt className="font-medium">Uncertainty</dt><dd>Model‑signaled confidence about its own output.</dd></div>
          <div><dt className="font-medium">Verification</dt><dd>Human checks that confirm claims with evidence.</dd></div>
        </dl>
      </div>

      <KeyTakeaways 
        points={[
          'AI hallucinations occur because it predicts the next word based on patterns, not facts.',
          'Provide specific context and grounding data to reduce hallucinations.',
          'Always verify critical information from primary sources.',
          'Use the I.N.S.Y.N.C. framework to ground the AI in your facts.'
        ]}
      />

      <section className="mt-6 bg-muted/30 border border-border rounded-xl p-4">
        <h4 className="text-sm font-semibold mb-2 text-foreground">You can now…</h4>
        <ul className="list-disc list-inside text-sm text-muted-foreground space-y-1">
          <li>Define hallucinations in plain language</li>
          <li>Identify common hallucination patterns and red flags</li>
          <li>Prompt with grounding context and structure to reduce errors</li>
          <li>Apply a verification step for important outputs</li>
          <li>List at least three red flags that suggest hallucination risk</li>
          <li>Design a simple verification plan for critical outputs</li>
        </ul>
      </section>
    </LessonTemplate>
  );
};

export default Lesson1_3;
